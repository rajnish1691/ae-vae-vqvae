{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJj8fj8KziJ4"
      },
      "source": [
        "# VQ-VAE Implementation in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  Vector Quantized VAE (VQ-VAE)\n",
        "# - Dataset: MNIST\n",
        "# - Train/Val split: 70/30 of the 60k \"train\" file\n",
        "# - Test: fixed 10k \"t10k\" file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import time, math, json, struct, gzip\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Config\n",
        "class QCfg:\n",
        "    # Paths\n",
        "    ROOT      = Path(\".\")\n",
        "    DATA_DIR  = ROOT / \"mnist\"\n",
        "    TRAIN_DIR = DATA_DIR / \"train\"\n",
        "    TEST_DIR  = DATA_DIR / \"test\"\n",
        "\n",
        "    # Splits\n",
        "    TRAIN_FRACTION = 0.70\n",
        "\n",
        "    # Training\n",
        "    BATCH_SIZE   = 256\n",
        "    EPOCHS       = 10\n",
        "    LR           = 1e-3\n",
        "    WEIGHT_DECAY = 1e-4\n",
        "    NUM_WORKERS  = 0\n",
        "    AMP          = False\n",
        "\n",
        "    # Early stopping\n",
        "    PATIENCE  = 3\n",
        "    MIN_DELTA = 1e-5\n",
        "\n",
        "    # VQ-VAE specific\n",
        "    CODEBOOK_SIZE = 256         # K (number of code vectors)\n",
        "    EMBED_DIM     = 64          # D (embedding/channel size in latent)\n",
        "    COMMIT_BETA   = 0.25        # β for commitment loss\n",
        "    EMA_DECAY     = 0.99        # EMA decay for codebook updates\n",
        "    EMA_EPS       = 1e-5\n",
        "\n",
        "    # Outputs\n",
        "    OUT_DIR  = ROOT / \"output\"\n",
        "    EXP_NAME = time.strftime(\"vqvae_%Y%m%d_%H%M%S\")\n",
        "\n",
        "    # Repro\n",
        "    SEED = 42\n",
        "\n",
        "qcfg = QCfg()\n",
        "\n",
        "# run folder\n",
        "vq_exp_dir = qcfg.OUT_DIR / qcfg.EXP_NAME\n",
        "vq_exp_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Device & seeds\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if device.type == \"cuda\":\n",
        "    qcfg.AMP = True\n",
        "\n",
        "torch.manual_seed(qcfg.SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(qcfg.SEED)\n",
        "\n",
        "print(\"Device     :\", device)\n",
        "print(\"Train dir  :\", qcfg.TRAIN_DIR.resolve())\n",
        "print(\"Test dir   :\", qcfg.TEST_DIR.resolve())\n",
        "print(\"Run folder :\", vq_exp_dir.resolve())\n",
        "print(f\"VQ-VAE: K={qcfg.CODEBOOK_SIZE}, D={qcfg.EMBED_DIM}, beta={qcfg.COMMIT_BETA}, ema_decay={qcfg.EMA_DECAY}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transforms\n",
        "from torchvision import transforms\n",
        "\n",
        "to_tensor_norm = transforms.Compose([\n",
        "    transforms.ToTensor(),                 # [0,1]\n",
        "    transforms.Normalize((0.5,), (0.5,))   # -> [-1,1]\n",
        "])\n",
        "inv_normalize = transforms.Normalize(mean=[-1*0.5/0.5], std=[1/0.5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset\n",
        "def _open_idx(path: Path):\n",
        "    # Supports .idx3-ubyte and .idx3-ubyte.gz\n",
        "    return gzip.open(path, \"rb\") if path.suffix == \".gz\" else open(path, \"rb\")\n",
        "\n",
        "def parse_idx_images(path: Path) -> np.ndarray:\n",
        "    \"\"\"Parse IDX image file (magic 2051) → numpy uint8 array (N, 28, 28).\"\"\"\n",
        "    with _open_idx(path) as f:\n",
        "        header = f.read(16)\n",
        "        if len(header) != 16:\n",
        "            raise RuntimeError(f\"Malformed IDX header in {path}\")\n",
        "        magic, num, rows, cols = struct.unpack(\">IIII\", header)\n",
        "        if magic != 2051:\n",
        "            raise RuntimeError(f\"Invalid IDX magic {magic} in {path} (expected 2051)\")\n",
        "        data = np.frombuffer(f.read(), dtype=np.uint8)\n",
        "    return data.reshape(num, rows, cols)\n",
        "\n",
        "class MNISTIdxDataset(Dataset):\n",
        "    \"\"\"Wraps (N,28,28) uint8 images; target == input (unsupervised).\"\"\"\n",
        "    def __init__(self, images: np.ndarray, transform=None):\n",
        "        self.images = images\n",
        "        self.transform = transform\n",
        "    def __len__(self): return len(self.images)\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.fromarray(self.images[idx], mode=\"L\")\n",
        "        x = self.transform(img) if self.transform else transforms.ToTensor()(img)\n",
        "        return x, x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# paths\n",
        "def find_idx_in_dir(dir_path: Path, candidates):\n",
        "    # check exact names first, then recurse\n",
        "    for fn in candidates:\n",
        "        p = dir_path / fn\n",
        "        if p.exists(): return p\n",
        "    for p in dir_path.rglob(\"*\"):\n",
        "        if p.is_file() and p.name in candidates:\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "train_candidates = [\n",
        "    \"train-images.idx3-ubyte\",\n",
        "    \"train-images-idx3-ubyte.gz\",\n",
        "]\n",
        "test_candidates = [\n",
        "    \"t10k-images.idx3-ubyte\",\n",
        "    \"t10k-images-idx3-ubyte.gz\",\n",
        "]\n",
        "\n",
        "train_dir = qcfg.TRAIN_DIR.resolve()\n",
        "test_dir  = qcfg.TEST_DIR.resolve()\n",
        "\n",
        "train_idx_path = find_idx_in_dir(train_dir, train_candidates)\n",
        "test_idx_path  = find_idx_in_dir(test_dir,  test_candidates)\n",
        "\n",
        "print(\"Train dir :\", train_dir)\n",
        "print(\"Test dir  :\", test_dir)\n",
        "print(\"Train IDX :\", train_idx_path)\n",
        "print(\"Test  IDX :\", test_idx_path)\n",
        "if train_idx_path is None:\n",
        "    raise RuntimeError(\"Could not find a train IDX in mnist/train. \"\n",
        "                       f\"Expected one of: {', '.join(train_candidates)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build datasets, split, and dataloaders\n",
        "g = torch.Generator().manual_seed(qcfg.SEED)\n",
        "\n",
        "# Load IDX arrays\n",
        "train_images = parse_idx_images(train_idx_path)\n",
        "print(\"Loaded train images:\", train_images.shape)\n",
        "\n",
        "test_images = None\n",
        "if test_idx_path is not None:\n",
        "    test_images = parse_idx_images(test_idx_path)\n",
        "    print(\"Loaded test images :\", test_images.shape)\n",
        "else:\n",
        "    print(\"No separate test IDX found — will split single file 70/15/15.\")\n",
        "\n",
        "# Datasets & splits\n",
        "if test_images is not None:\n",
        "    full_ds = MNISTIdxDataset(train_images, transform=to_tensor_norm)\n",
        "    test_ds = MNISTIdxDataset(test_images,  transform=to_tensor_norm)\n",
        "\n",
        "    n_full  = len(full_ds)                      # 60_000\n",
        "    n_train = int(qcfg.TRAIN_FRACTION * n_full) # 42_000 (70%)\n",
        "    n_val   = n_full - n_train                  # 18_000 (30%)\n",
        "    train_ds, val_ds = random_split(full_ds, [n_train, n_val], generator=g)\n",
        "else:\n",
        "    full_ds = MNISTIdxDataset(train_images, transform=to_tensor_norm)\n",
        "    n_full  = len(full_ds)\n",
        "    n_train = int(qcfg.TRAIN_FRACTION * n_full) # 70%\n",
        "    n_rest  = n_full - n_train                  # 30%\n",
        "    n_val   = n_rest // 2                       # 15%\n",
        "    n_test  = n_rest - n_val                    # 15%\n",
        "    train_ds, val_ds, test_ds = random_split(full_ds, [n_train, n_val, n_test], generator=g)\n",
        "\n",
        "print(f\"Final splits → Train: {len(train_ds)} | Val: {len(val_ds)} | Test: {len(test_ds)}\")\n",
        "\n",
        "# DataLoaders\n",
        "pin = (device.type == \"cuda\")\n",
        "train_loader = DataLoader(train_ds, batch_size=qcfg.BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=qcfg.NUM_WORKERS, pin_memory=pin)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=qcfg.BATCH_SIZE, shuffle=False,\n",
        "                          num_workers=qcfg.NUM_WORKERS, pin_memory=pin)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=qcfg.BATCH_SIZE, shuffle=False,\n",
        "                          num_workers=qcfg.NUM_WORKERS, pin_memory=pin)\n",
        "\n",
        "print(\"pin_memory:\", pin, \"| batch_size:\", qcfg.BATCH_SIZE, \"| workers:\", qcfg.NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick sanity Check\n",
        "xb, _ = next(iter(train_loader))\n",
        "xb_vis = inv_normalize(xb[:8].cpu()).clamp(0,1)\n",
        "grid = make_grid(xb_vis, nrow=8, padding=2)\n",
        "plt.figure(figsize=(8,2))\n",
        "plt.axis('off'); plt.title(\"VQ-VAE — sample training images\")\n",
        "plt.imshow(np.transpose(grid.numpy(), (1,2,0)), cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# VQ-VAE model\n",
        "# - Encoder downsamples 28→14→7 and outputs D=qcfg.EMBED_DIM channels.\n",
        "# - VectorQuantizer **EMA** codebook (K=qcfg.CODEBOOK_SIZE, D channels).\n",
        "# - Straight-through estimator for the backward pass.\n",
        "# - Decoder upsamples 7→14→28 and outputs tanh in [-1, 1].\n",
        "# - Forward returns: reconstruction, commitment loss, perplexity, and code indices.\n",
        "\n",
        "import math\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, cin, cout, gn_groups=8):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(cin, cout, 3, padding=1),\n",
        "            nn.GroupNorm(min(gn_groups, cout), cout),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(cout, cout, 3, padding=1),\n",
        "            nn.GroupNorm(min(gn_groups, cout), cout),\n",
        "            nn.SiLU(),\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"ConvBlock then stride-2 downsample (28→14 or 14→7).\"\"\"\n",
        "    def __init__(self, cin, cout):\n",
        "        super().__init__()\n",
        "        self.block = ConvBlock(cin, cout)\n",
        "        self.down  = nn.Conv2d(cout, cout, kernel_size=2, stride=2)\n",
        "    def forward(self, x):\n",
        "        x = self.block(x)\n",
        "        x = self.down(x)\n",
        "        return x\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Stride-2 upsample then ConvBlock (7→14 or 14→28).\"\"\"\n",
        "    def __init__(self, cin, cout):\n",
        "        super().__init__()\n",
        "        self.up   = nn.ConvTranspose2d(cin, cout, kernel_size=2, stride=2)\n",
        "        self.block= ConvBlock(cout, cout)\n",
        "    def forward(self, x):\n",
        "        x = self.up(x)\n",
        "        x = self.block(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# VectorQuantizer with EMA codebook updates\n",
        "class VectorQuantizerEMA(nn.Module):\n",
        "    \"\"\"\n",
        "    VQ layer with EMA codebook updates (no explicit codebook loss).\n",
        "    - K: number of code vectors (codebook size)\n",
        "    - D: embedding dimension (must match encoder channels)\n",
        "    - decay: EMA decay for cluster sizes and embedding averages\n",
        "    - eps: small constant for numerical stability\n",
        "    - beta_commit: scales the commitment loss ||sg[z_e] - z_q||^2\n",
        "    Returns:\n",
        "      z_q (quantized), commit_loss (scalar), perplexity (scalar), indices (B,H,W)\n",
        "    \"\"\"\n",
        "    def __init__(self, K, D, decay=0.99, eps=1e-5, beta_commit=0.25):\n",
        "        super().__init__()\n",
        "        self.K = K\n",
        "        self.D = D\n",
        "        self.decay = decay\n",
        "        self.eps = eps\n",
        "        self.beta = beta_commit\n",
        "\n",
        "        embed = torch.randn(K, D) / D**0.5\n",
        "        self.register_buffer(\"embedding\", embed)           # (K, D)\n",
        "        self.register_buffer(\"cluster_size\", torch.zeros(K))\n",
        "        self.register_buffer(\"embed_avg\", embed.clone())   # EMA of embeddings\n",
        "\n",
        "    def forward(self, z_e):\n",
        "        \"\"\"\n",
        "        z_e: encoder outputs, shape (B, D, H, W)\n",
        "        \"\"\"\n",
        "        B, D, H, W = z_e.shape\n",
        "        assert D == self.D, f\"Embed dim mismatch: got {D}, expected {self.D}\"\n",
        "\n",
        "        # Flatten to (N, D)\n",
        "        z = z_e.permute(0, 2, 3, 1).contiguous()      # (B,H,W,D)\n",
        "        flat = z.view(-1, D)                          # (N,D), N=B*H*W\n",
        "\n",
        "        # Squared L2 distances to codebook: ||x - e||^2 = x^2 + e^2 - 2 x·e\n",
        "        # shapes: flat (N,D), embedding (K,D)\n",
        "        dist = (\n",
        "            flat.pow(2).sum(1, keepdim=True)          # (N,1)\n",
        "            + self.embedding.pow(2).sum(1)            # (K,)\n",
        "            - 2 * flat @ self.embedding.t()           # (N,K)\n",
        "        )\n",
        "\n",
        "        # Encoding indices and one-hot\n",
        "        indices = torch.argmin(dist, dim=1)           # (N,)\n",
        "        encodings = F.one_hot(indices, self.K).type(flat.dtype)  # (N,K)\n",
        "\n",
        "        # Quantize and reshape back\n",
        "        quant = encodings @ self.embedding            # (N,D)\n",
        "        quant = quant.view(B, H, W, D).permute(0, 3, 1, 2).contiguous()  # (B,D,H,W)\n",
        "\n",
        "        # EMA updates (training only)\n",
        "        if self.training:\n",
        "            # EMA cluster size\n",
        "            new_cluster_size = encodings.sum(0)       # (K,)\n",
        "            self.cluster_size.mul_(self.decay).add_( (1 - self.decay) * new_cluster_size )\n",
        "\n",
        "            # EMA embedding average\n",
        "            embed_sum = encodings.t() @ flat          # (K,D)\n",
        "            self.embed_avg.mul_(self.decay).add_( (1 - self.decay) * embed_sum )\n",
        "\n",
        "            # Normalize to get updated embeddings\n",
        "            n = self.cluster_size.sum()\n",
        "            # Laplace smoothing of cluster sizes\n",
        "            cluster_size = (self.cluster_size + self.eps) / (n + self.K * self.eps) * n\n",
        "            embed_normalized = self.embed_avg / cluster_size.unsqueeze(1)\n",
        "            self.embedding.copy_(embed_normalized)\n",
        "\n",
        "        # Commitment loss (EMA variant doesn’t use separate codebook loss)\n",
        "        commit_loss = self.beta * F.mse_loss(quant.detach(), z_e)\n",
        "\n",
        "        # Straight-through estimator\n",
        "        z_q = z_e + (quant - z_e).detach()\n",
        "\n",
        "        # Perplexity over usage of codebook entries\n",
        "        avg_probs = encodings.float().mean(0) + 1e-10\n",
        "        perplexity = torch.exp(-(avg_probs * avg_probs.log()).sum())\n",
        "\n",
        "        # Index map (B,H,W)\n",
        "        idx_map = indices.view(B, H, W)\n",
        "\n",
        "        return z_q, commit_loss, perplexity, idx_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# VQ-VAE encoder / decoder / model\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1),\n",
        "            nn.GroupNorm(8, 32), nn.SiLU(),\n",
        "            nn.Conv2d(32, 32, 3, padding=1),\n",
        "            nn.GroupNorm(8, 32), nn.SiLU(),\n",
        "\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 28 → 14\n",
        "            nn.GroupNorm(8, 64), nn.SiLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.GroupNorm(8, 64), nn.SiLU(),\n",
        "\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1), # 14 → 7\n",
        "            nn.GroupNorm(16, 128), nn.SiLU(),\n",
        "            nn.Conv2d(128, embed_dim, 1),               # project to D\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)           # (B,D,7,7)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(embed_dim, 128, 3, padding=1),\n",
        "            nn.GroupNorm(16, 128), nn.SiLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 64, 2, stride=2),  # 7 → 14\n",
        "            nn.GroupNorm(8, 64), nn.SiLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.GroupNorm(8, 64), nn.SiLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 32, 2, stride=2),   # 14 → 28\n",
        "            nn.GroupNorm(8, 32), nn.SiLU(),\n",
        "            nn.Conv2d(32, 16, 3, padding=1),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(16, 1, 1),\n",
        "            nn.Tanh(),                                  # [-1, 1]\n",
        "        )\n",
        "    def forward(self, z_q): return self.net(z_q)\n",
        "\n",
        "class VQVAE(nn.Module):\n",
        "    def __init__(self, K, D, beta_commit=0.25, ema_decay=0.99, ema_eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(D)\n",
        "        self.quant   = VectorQuantizerEMA(K, D, decay=ema_decay, eps=ema_eps, beta_commit=beta_commit)\n",
        "        self.decoder = Decoder(D)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z_e = self.encoder(x)                                    # (B,D,7,7)\n",
        "        z_q, commit_loss, perplexity, idx = self.quant(z_e)      # quantize\n",
        "        xhat = self.decoder(z_q)                                  # reconstruct\n",
        "        info = {\"commit_loss\": commit_loss, \"perplexity\": perplexity, \"indices\": idx}\n",
        "        return xhat, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instantiate and quick shape sanity check\n",
        "vqvae = VQVAE(\n",
        "    K=qcfg.CODEBOOK_SIZE,\n",
        "    D=qcfg.EMBED_DIM,\n",
        "    beta_commit=qcfg.COMMIT_BETA,\n",
        "    ema_decay=qcfg.EMA_DECAY,\n",
        "    ema_eps=qcfg.EMA_EPS,\n",
        ").to(device)\n",
        "\n",
        "print(\"VQ-VAE params (M):\", sum(p.numel() for p in vqvae.parameters())/1e6)\n",
        "\n",
        "vqvae.eval()\n",
        "xb, _ = next(iter(train_loader))\n",
        "xb = xb.to(device)[:8]\n",
        "with torch.no_grad():\n",
        "    xhat, info = vqvae(xb)\n",
        "print(\"Input :\", tuple(xb.shape))\n",
        "print(\"Recon :\", tuple(xhat.shape))\n",
        "print(\"Commit loss (scalar):\", float(info[\"commit_loss\"]))\n",
        "print(\"Perplexity (scalar) :\", float(info[\"perplexity\"]))\n",
        "assert xhat.shape == xb.shape, \"Decoder must reconstruct to (N,1,28,28)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training\n",
        "# - Loss = reconstruction + commitment (EMA variant; no separate codebook loss)\n",
        "# - Early stopping on **val total loss**\n",
        "# - Saves:\n",
        "#     - `best_model.pt`  (weights only)\n",
        "#     - `best_val.ckpt`  (full checkpoint with optimizer state)\n",
        "#     - `last.ckpt`      (final state)\n",
        "#     - `history.json`   (training curves)\n",
        "\n",
        "# Optimizer / scheduler / scaler\n",
        "optimizer = torch.optim.AdamW(vqvae.parameters(), lr=qcfg.LR, weight_decay=qcfg.WEIGHT_DECAY)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "    optimizer, T_0=max(5, qcfg.EPOCHS), T_mult=1\n",
        ")\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=qcfg.AMP)\n",
        "\n",
        "best_path_weights = vq_exp_dir / \"best_model.pt\"\n",
        "best_path_ckpt    = vq_exp_dir / \"best_val.ckpt\"\n",
        "last_path_ckpt    = vq_exp_dir / \"last.ckpt\"\n",
        "\n",
        "def save_ckpt(path, model, optimizer, epoch, metrics: dict):\n",
        "    torch.save({\n",
        "        \"epoch\": epoch,\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "        \"metrics\": metrics,\n",
        "        \"cfg\": vars(qcfg),\n",
        "    }, path)\n",
        "\n",
        "def recon_loss(xhat, x):\n",
        "    # same hybrid as AE/VAE\n",
        "    l1 = F.l1_loss(xhat, x)\n",
        "    l2 = F.mse_loss(xhat, x)\n",
        "    return 0.5*l1 + 0.5*l2, l1.item(), l2.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train\n",
        "import numpy as np\n",
        "\n",
        "best_val = float(\"inf\")\n",
        "no_improve = 0\n",
        "history = {\n",
        "    \"train_total\": [], \"train_recon\": [], \"train_commit\": [], \"train_perplex\": [],\n",
        "    \"val_total\":   [], \"val_recon\":   [], \"val_commit\":   [], \"val_perplex\":   []\n",
        "}\n",
        "\n",
        "for epoch in range(1, qcfg.EPOCHS + 1):\n",
        "    # ---- Train ----\n",
        "    vqvae.train()\n",
        "    t_rec_vals, t_com_vals, t_tot_vals, t_ppl_vals = [], [], [], []\n",
        "    for xb, _ in train_loader:\n",
        "        xb = xb.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=qcfg.AMP):\n",
        "            xhat, info = vqvae(xb)\n",
        "            rec, _, _ = recon_loss(xhat, xb)\n",
        "            commit = info[\"commit_loss\"]\n",
        "            loss = rec + commit\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(vqvae.parameters(), max_norm=1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        t_rec_vals.append(rec.item())\n",
        "        t_com_vals.append(float(commit))\n",
        "        t_tot_vals.append(loss.item())\n",
        "        t_ppl_vals.append(float(info[\"perplexity\"]))\n",
        "    scheduler.step(epoch)\n",
        "\n",
        "    # ---- Validate ----\n",
        "    vqvae.eval()\n",
        "    v_rec_vals, v_com_vals, v_tot_vals, v_ppl_vals = [], [], [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, _ in val_loader:\n",
        "            xb = xb.to(device)\n",
        "            xhat, info = vqvae(xb)\n",
        "            rec, _, _ = recon_loss(xhat, xb)\n",
        "            commit = info[\"commit_loss\"]\n",
        "            total = rec + commit\n",
        "            v_rec_vals.append(rec.item())\n",
        "            v_com_vals.append(float(commit))\n",
        "            v_tot_vals.append(total.item())\n",
        "            v_ppl_vals.append(float(info[\"perplexity\"]))\n",
        "\n",
        "    train_recon = float(np.mean(t_rec_vals)) if t_rec_vals else float(\"inf\")\n",
        "    train_commit= float(np.mean(t_com_vals)) if t_com_vals else float(\"inf\")\n",
        "    train_total = float(np.mean(t_tot_vals)) if t_tot_vals else float(\"inf\")\n",
        "    train_ppl   = float(np.mean(t_ppl_vals)) if t_ppl_vals else float(\"nan\")\n",
        "\n",
        "    val_recon   = float(np.mean(v_rec_vals)) if v_rec_vals else float(\"inf\")\n",
        "    val_commit  = float(np.mean(v_com_vals)) if v_com_vals else float(\"inf\")\n",
        "    val_total   = float(np.mean(v_tot_vals)) if v_tot_vals else float(\"inf\")\n",
        "    val_ppl     = float(np.mean(v_ppl_vals)) if v_ppl_vals else float(\"nan\")\n",
        "\n",
        "    history[\"train_total\"].append(train_total)\n",
        "    history[\"train_recon\"].append(train_recon)\n",
        "    history[\"train_commit\"].append(train_commit)\n",
        "    history[\"train_perplex\"].append(train_ppl)\n",
        "    history[\"val_total\"].append(val_total)\n",
        "    history[\"val_recon\"].append(val_recon)\n",
        "    history[\"val_commit\"].append(val_commit)\n",
        "    history[\"val_perplex\"].append(val_ppl)\n",
        "\n",
        "    print(f\"Epoch {epoch:03d} | \"\n",
        "          f\"train total {train_total:.5f} (rec {train_recon:.5f}, com {train_commit:.5f}, ppl {train_ppl:.1f}) | \"\n",
        "          f\"val total {val_total:.5f} (rec {val_recon:.5f}, com {val_commit:.5f}, ppl {val_ppl:.1f})\")\n",
        "\n",
        "    # ---- Early stopping & best checkpoint ----\n",
        "    if best_val - val_total > qcfg.MIN_DELTA:\n",
        "        best_val = val_total\n",
        "        no_improve = 0\n",
        "        save_ckpt(best_path_ckpt, vqvae, optimizer, epoch,\n",
        "                  {\"val_total\": val_total, \"val_recon\": val_recon, \"val_commit\": val_commit, \"val_perplex\": val_ppl})\n",
        "        torch.save(vqvae.state_dict(), best_path_weights)\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= qcfg.PATIENCE:\n",
        "            print(\"Early stopping.\")\n",
        "            break\n",
        "\n",
        "# Save last & history\n",
        "save_ckpt(last_path_ckpt, vqvae, optimizer, epoch, {\"val_total\": val_total})\n",
        "with open(vq_exp_dir / \"history.json\", \"w\") as f:\n",
        "    json.dump(history, f, indent=2)\n",
        "\n",
        "# Load best weights for downstream eval\n",
        "vqvae.load_state_dict(torch.load(best_path_weights, map_location=device))\n",
        "vqvae.eval()\n",
        "print(\"Loaded best weights from:\", best_path_weights.resolve())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "# - Test metrics: reconstruction, commitment, total (= recon + commit), plus Relative L1 / L2\n",
        "# - Codebook: mean perplexity, usage histogram (how many codes got used)\n",
        "\n",
        "# Ensure best weights are loaded\n",
        "try:\n",
        "    best_path_weights\n",
        "except NameError:\n",
        "    best_path_weights = vq_exp_dir / \"best_model.pt\"\n",
        "\n",
        "vqvae.load_state_dict(torch.load(best_path_weights, map_location=device))\n",
        "vqvae.eval()\n",
        "print(\"Loaded best:\", best_path_weights.resolve())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helpers: losses + relative errors + code prior tools\n",
        "import json\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def recon_loss(xhat, x):\n",
        "    l1 = F.l1_loss(xhat, x)\n",
        "    l2 = F.mse_loss(xhat, x)\n",
        "    return 0.5*l1 + 0.5*l2, l1.item(), l2.item()\n",
        "\n",
        "def per_sample_rel_errors(xhat, x, eps=1e-12):\n",
        "    diff = (xhat - x)\n",
        "    relL1 = diff.abs().flatten(1).sum(1) / (x.abs().flatten(1).sum(1) + eps)\n",
        "    relL2 = torch.sqrt((diff**2).flatten(1).sum(1)) / (torch.sqrt((x**2).flatten(1).sum(1) + eps))\n",
        "    return relL1, relL2\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_code_usage(dloader, model, K):\n",
        "    \"\"\"Return normalized histogram over codes and mean perplexity on a loader.\"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    counts = torch.zeros(K, device=device)\n",
        "    ppl_vals = []\n",
        "    for xb, _ in dloader:\n",
        "        xb = xb.to(device)\n",
        "        _, info = model(xb)\n",
        "        idx = info[\"indices\"]             # (B,H,W)\n",
        "        counts.index_add_(0, idx.view(-1), torch.ones(idx.numel(), device=device))\n",
        "        ppl_vals.append(float(info[\"perplexity\"]))\n",
        "    counts = counts + 1e-8                # avoid zeros\n",
        "    probs = counts / counts.sum()\n",
        "    mean_ppl = float(np.mean(ppl_vals)) if ppl_vals else float(\"nan\")\n",
        "    return probs, mean_ppl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test metrics (recon / commit / total) + Relative L1/L2\n",
        "recon_vals, commit_vals, total_vals = [], [], []\n",
        "relL1_vals, relL2_vals = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, _ in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        xhat, info = vqvae(xb)\n",
        "        rec, _, _ = recon_loss(xhat, xb)\n",
        "        com = info[\"commit_loss\"]\n",
        "        tot = rec + com\n",
        "\n",
        "        r1, r2 = per_sample_rel_errors(xhat, xb)\n",
        "\n",
        "        recon_vals.append(rec.item())\n",
        "        commit_vals.append(float(com))\n",
        "        total_vals.append(tot.item())\n",
        "        relL1_vals.append(r1.cpu()); relL2_vals.append(r2.cpu())\n",
        "\n",
        "recon_mean = float(np.mean(recon_vals))\n",
        "commit_mean = float(np.mean(commit_vals))\n",
        "total_mean = float(np.mean(total_vals))\n",
        "relL1_mean = torch.cat(relL1_vals).mean().item()\n",
        "relL2_mean = torch.cat(relL2_vals).mean().item()\n",
        "\n",
        "print(f\"[TEST] recon: {recon_mean:.6f} | commit: {commit_mean:.6f} | total: {total_mean:.6f}\")\n",
        "print(f\"[TEST] Relative L1: {relL1_mean:.6f} | Relative L2: {relL2_mean:.6f}\")\n",
        "\n",
        "with open(vq_exp_dir / \"test_metrics.json\", \"w\") as f:\n",
        "    json.dump({\n",
        "        \"recon\": recon_mean, \"commit\": commit_mean, \"total\": total_mean,\n",
        "        \"relL1\": relL1_mean, \"relL2\": relL2_mean\n",
        "    }, f, indent=2)\n",
        "print(\"Saved metrics to:\", (vq_exp_dir / \"test_metrics.json\").resolve())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Codebook stats: usage prior (train) + perplexity\n",
        "K = qcfg.CODEBOOK_SIZE\n",
        "train_prior, train_ppl = estimate_code_usage(train_loader, vqvae, K)\n",
        "test_prior,  test_ppl  = estimate_code_usage(test_loader,  vqvae, K)\n",
        "\n",
        "used_codes_train = int((train_prior > (1.0 / (train_prior.numel()*1e6))).sum().item())\n",
        "used_codes_test  = int((test_prior  > (1.0 / (test_prior.numel()*1e6))).sum().item())\n",
        "\n",
        "print(f\"[CODEBOOK] train perplexity ~ {train_ppl:.2f} | used codes: {used_codes_train}/{K}\")\n",
        "print(f\"[CODEBOOK] test  perplexity ~ {test_ppl:.2f}  | used codes: {used_codes_test}/{K}\")\n",
        "\n",
        "torch.save({\"train_prior\": train_prior.cpu(), \"test_prior\": test_prior.cpu(),\n",
        "            \"train_ppl\": train_ppl, \"test_ppl\": test_ppl},\n",
        "           vq_exp_dir / \"codebook_stats.pt\")\n",
        "print(\"Saved codebook stats to:\", (vq_exp_dir / \"codebook_stats.pt\").resolve())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Worst / Median / Best reconstructions by Relative L2\n",
        "images, recons, errs = [], [], []\n",
        "with torch.no_grad():\n",
        "    for xb, _ in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        xhat, _ = vqvae(xb)\n",
        "        _, r2 = per_sample_rel_errors(xhat, xb)\n",
        "        images.append(xb.cpu()); recons.append(xhat.cpu()); errs.append(r2.cpu())\n",
        "\n",
        "images = torch.cat(images)   # (N,1,28,28)\n",
        "recons = torch.cat(recons)\n",
        "errs   = torch.cat(errs)\n",
        "\n",
        "worst_idx  = torch.argmax(errs).item()\n",
        "best_idx   = torch.argmin(errs).item()\n",
        "median_val = torch.median(errs).item()\n",
        "median_idx = (errs - median_val).abs().argmin().item()\n",
        "print(\"Indices → BEST:\", best_idx, \"| MEDIAN:\", median_idx, \"| WORST:\", worst_idx)\n",
        "\n",
        "def show_pair(orig, recon, title, save_path=None):\n",
        "    o = inv_normalize(orig).clamp(0,1).squeeze(0)   # (H,W)\n",
        "    r = inv_normalize(recon).clamp(0,1).squeeze(0)  # (H,W)\n",
        "    both = torch.cat([o, r], dim=-1)\n",
        "    plt.figure(figsize=(3,3))\n",
        "    plt.axis('off'); plt.title(title)\n",
        "    plt.imshow(both, cmap='gray')\n",
        "    if save_path is not None:\n",
        "        plt.savefig(save_path, bbox_inches='tight', dpi=160)\n",
        "    plt.show()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for label, idx in [(\"BEST\", best_idx), (\"MEDIAN\", median_idx), (\"WORST\", worst_idx)]:\n",
        "        x  = images[idx:idx+1]\n",
        "        xr = recons[idx:idx+1]\n",
        "        r1, r2 = per_sample_rel_errors(xr, x)\n",
        "        fp = vq_exp_dir / f\"vqvae_test_{label.lower()}_pair.png\"\n",
        "        show_pair(x[0], xr[0], f\"{label} — RelL1={r1.item():.4f} | RelL2={r2.item():.4f}\", fp)\n",
        "        print(f\"Saved {label.lower()} pair to:\", fp.resolve())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sampling from code priors (empirical & uniform)\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "@torch.no_grad()\n",
        "def decode_from_indices(idx_map, codebook):\n",
        "    \"\"\"\n",
        "    idx_map: (B, H, W) long tensor of code indices\n",
        "    codebook: (K, D) tensor of embeddings\n",
        "    returns z_q: (B, D, H, W)\n",
        "    \"\"\"\n",
        "    B, H, W = idx_map.shape\n",
        "    D = codebook.shape[1]\n",
        "    flat = idx_map.view(B, -1)                       # (B, H*W)\n",
        "    emb = codebook.index_select(0, flat.view(-1))    # (B*H*W, D)\n",
        "    emb = emb.view(B, H, W, D).permute(0, 3, 1, 2).contiguous()\n",
        "    return emb\n",
        "\n",
        "# Prepare priors\n",
        "H = W = 7\n",
        "n = 64  # 8x8 grid of samples\n",
        "\n",
        "# 1) Empirical prior (from train usage)\n",
        "train_probs = train_prior.to(device)\n",
        "emp_idx = torch.multinomial(train_probs, num_samples=n*H*W, replacement=True).view(n, H, W)\n",
        "\n",
        "# 2) Uniform prior\n",
        "uni_idx = torch.randint(low=0, high=K, size=(n, H, W), device=device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    zq_emp = decode_from_indices(emp_idx, vqvae.quant.embedding)\n",
        "    zq_uni = decode_from_indices(uni_idx, vqvae.quant.embedding)\n",
        "    samp_emp = vqvae.decoder(zq_emp).cpu()\n",
        "    samp_uni = vqvae.decoder(zq_uni).cpu()\n",
        "\n",
        "    samp_emp_vis = inv_normalize(samp_emp).clamp(0,1)\n",
        "    samp_uni_vis = inv_normalize(samp_uni).clamp(0,1)\n",
        "\n",
        "grid_emp = make_grid(samp_emp_vis, nrow=8, padding=2)\n",
        "grid_uni = make_grid(samp_uni_vis, nrow=8, padding=2)\n",
        "\n",
        "plt.figure(figsize=(6,6)); plt.axis('off'); plt.title(\"VQ-VAE samples — empirical code prior\")\n",
        "plt.imshow(np.transpose(grid_emp.numpy(), (1,2,0))); plt.show()\n",
        "plt.figure(figsize=(6,6)); plt.axis('off'); plt.title(\"VQ-VAE samples — uniform code prior\")\n",
        "plt.imshow(np.transpose(grid_uni.numpy(), (1,2,0))); plt.show()\n",
        "\n",
        "fp_emp = vq_exp_dir / \"vqvae_samples_empirical.png\"\n",
        "fp_uni = vq_exp_dir / \"vqvae_samples_uniform.png\"\n",
        "plt.imsave(fp_emp, np.transpose(grid_emp.numpy(), (1,2,0)))\n",
        "plt.imsave(fp_uni, np.transpose(grid_uni.numpy(), (1,2,0)))\n",
        "print(\"Saved samples to:\\n \", fp_emp.resolve(), \"\\n \", fp_uni.resolve())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ae-vae-vqvae",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
