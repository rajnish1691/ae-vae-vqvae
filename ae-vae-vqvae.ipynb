{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJj8fj8KziJ4"
      },
      "source": [
        "# [AE-VAE-VQVAE] Implementation in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Autoencoder (AE)\n",
        "# - Dataset: MNIST\n",
        "# - Train/Val split: 70/30 of the 60k \"train\" file\n",
        "# - Test: fixed 10k \"t10k\" file\n",
        "# - Conv AE, AdamW, cosine schedule, AMP, early stopping\n",
        "# - Saves best-by-val-RelL2\n",
        "# - Evaluates Relative L1/L2 on test\n",
        "# - Visualizes worst/best/median test reconstructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import struct, time, math, json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%pip install -q \"numpy<2.0\"\n",
        "import numpy as np\n",
        "print(\"NumPy version:\", np.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A.1 Config\n",
        "class Cfg:\n",
        "    ROOT = Path(\".\")\n",
        "\n",
        "    DATA_DIR  = ROOT / \"mnist\"\n",
        "    TRAIN_DIR = DATA_DIR / \"train\"\n",
        "    TEST_DIR  = DATA_DIR / \"test\"\n",
        "\n",
        "    # Split\n",
        "    TRAIN_FRACTION = 0.70\n",
        "\n",
        "    # Training\n",
        "    BATCH_SIZE   = 256\n",
        "    EPOCHS       = 10\n",
        "    LR           = 1e-3\n",
        "    WEIGHT_DECAY = 1e-4\n",
        "    NUM_WORKERS  = 2\n",
        "    AMP          = True\n",
        "\n",
        "    # Early stopping\n",
        "    PATIENCE = 2\n",
        "    MIN_DELTA = 1e-5\n",
        "    \n",
        "    # Outputs\n",
        "    OUT_DIR  = ROOT / \"output\"\n",
        "    EXP_NAME = time.strftime(\"ae_%Y%m%d_%H%M%S\")\n",
        "\n",
        "    # Repro\n",
        "    SEED = 42\n",
        "\n",
        "cfg = Cfg()\n",
        "\n",
        "# run folder for saving\n",
        "exp_dir = cfg.OUT_DIR / cfg.EXP_NAME\n",
        "exp_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Device & seeds\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(cfg.SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(cfg.SEED)\n",
        "\n",
        "print(\"Device     :\", device)\n",
        "print(\"Train dir  :\", cfg.TRAIN_DIR.resolve())\n",
        "print(\"Test dir   :\", cfg.TEST_DIR.resolve())\n",
        "print(\"Run folder :\", exp_dir.resolve())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A.2 Transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "inv_normalize = transforms.Normalize(mean=[-1*0.5/0.5], std=[1/0.5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A.3 IDX helpers and dataset\n",
        "def _open_idx(path: Path):\n",
        "    \"\"\"Opens .idx or .idx.gz transparently.\"\"\"\n",
        "    return gzip.open(path, \"rb\") if path.suffix == \".gz\" else open(path, \"rb\")\n",
        "\n",
        "def parse_idx_images(path: Path) -> np.ndarray:\n",
        "    \"\"\"Parse IDX image file (magic 2051) → numpy uint8 array (N, 28, 28).\"\"\"\n",
        "    with _open_idx(path) as f:\n",
        "        header = f.read(16)\n",
        "        if len(header) != 16:\n",
        "            raise RuntimeError(f\"Malformed IDX header in {path}\")\n",
        "        magic, num, rows, cols = struct.unpack(\">IIII\", header)\n",
        "        if magic != 2051:\n",
        "            raise RuntimeError(f\"Invalid IDX magic {magic} in {path} (expected 2051)\")\n",
        "        data = np.frombuffer(f.read(), dtype=np.uint8)\n",
        "    return data.reshape(num, rows, cols)\n",
        "\n",
        "class MNISTIdxDataset(Dataset):\n",
        "    \"\"\"Wraps (N,28,28) uint8 arrays as a torch Dataset; target == input (AE).\"\"\"\n",
        "    def __init__(self, images: np.ndarray, transform=None):\n",
        "        self.images = images\n",
        "        self.transform = transform\n",
        "    def __len__(self): return len(self.images)\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.fromarray(self.images[idx], mode=\"L\")\n",
        "        x = self.transform(img) if self.transform else transforms.ToTensor()(img)\n",
        "        return x, x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A.4 IDX files\n",
        "train_candidates = [\n",
        "    \"train-images.idx3-ubyte\",\n",
        "    \"train-images-idx3-ubyte.gz\",\n",
        "]\n",
        "test_candidates = [\n",
        "    \"t10k-images.idx3-ubyte\",\n",
        "    \"t10k-images-idx3-ubyte.gz\",\n",
        "]\n",
        "\n",
        "def find_idx_in_dir(dir_path: Path, candidates):\n",
        "    for fn in candidates:\n",
        "        p = dir_path / fn\n",
        "        if p.exists(): return p\n",
        "    for p in dir_path.rglob(\"*\"):\n",
        "        if p.is_file() and p.name in candidates:\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "train_dir = (cfg.TRAIN_DIR).resolve()\n",
        "test_dir  = (cfg.TEST_DIR).resolve()\n",
        "\n",
        "train_idx_path = find_idx_in_dir(train_dir, train_candidates)\n",
        "test_idx_path  = find_idx_in_dir(test_dir,  test_candidates)\n",
        "\n",
        "print(\"Train dir :\", train_dir)\n",
        "print(\"Test dir  :\", test_dir)\n",
        "print(\"Train IDX :\", train_idx_path)\n",
        "print(\"Test  IDX :\", test_idx_path)\n",
        "\n",
        "if train_idx_path is None:\n",
        "    raise RuntimeError(\"Could not find a train IDX in mnist/train. \"\n",
        "                       f\"Expected one of: {', '.join(train_candidates)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A.5 Load arrays, build splits, create dataloaders\n",
        "g = torch.Generator().manual_seed(cfg.SEED)\n",
        "\n",
        "# Load training IDX\n",
        "train_images = parse_idx_images(train_idx_path)\n",
        "print(\"Loaded train images:\", train_images.shape)\n",
        "\n",
        "# Load test IDX if present\n",
        "test_images = None\n",
        "if test_idx_path is not None:\n",
        "    test_images = parse_idx_images(test_idx_path)\n",
        "    print(\"Loaded test images :\", test_images.shape)\n",
        "else:\n",
        "    print(\"No test IDX found\")\n",
        "\n",
        "# Build datasets/splits\n",
        "if test_images is not None:\n",
        "    full_ds = MNISTIdxDataset(train_images, transform=transform)\n",
        "    test_ds = MNISTIdxDataset(test_images,  transform=transform)\n",
        "\n",
        "    n_full  = len(full_ds)                 #  60_000\n",
        "    n_train = int(cfg.TRAIN_FRACTION * n_full)   # 70% of 60k = 42k by default\n",
        "    n_val   = n_full - n_train                    # 18k\n",
        "    train_ds, val_ds = random_split(full_ds, [n_train, n_val], generator=g)\n",
        "else:\n",
        "    # Fallback: only one IDX → 70/15/15\n",
        "    full_ds = MNISTIdxDataset(train_images, transform=transform)\n",
        "    n_full  = len(full_ds)\n",
        "    n_train = int(cfg.TRAIN_FRACTION * n_full)   # 70%\n",
        "    n_rest  = n_full - n_train                   # 30%\n",
        "    n_val   = n_rest // 2                        # 15%\n",
        "    n_test  = n_rest - n_val                     # 15%\n",
        "    train_ds, val_ds, test_ds = random_split(full_ds, [n_train, n_val, n_test], generator=g)\n",
        "\n",
        "print(f\"Final splits → Train: {len(train_ds)} | Val: {len(val_ds)} | Test: {len(test_ds)}\")\n",
        "\n",
        "# DataLoaders\n",
        "pin = True\n",
        "train_loader = DataLoader(train_ds, batch_size=cfg.BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=cfg.NUM_WORKERS, pin_memory=pin)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=cfg.BATCH_SIZE, shuffle=False,\n",
        "                          num_workers=cfg.NUM_WORKERS, pin_memory=pin)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=cfg.BATCH_SIZE, shuffle=False,\n",
        "                          num_workers=cfg.NUM_WORKERS, pin_memory=pin)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A.6 Quick sanity check\n",
        "xb, _ = next(iter(train_loader))\n",
        "xb_vis = inv_normalize(xb[:8].cpu()).clamp(0,1)\n",
        "grid = make_grid(xb_vis, nrow=8, padding=2)\n",
        "plt.figure(figsize=(8,2))\n",
        "plt.axis('off')\n",
        "plt.title(\"Sample training images (after normalization inverse)\")\n",
        "plt.imshow(np.transpose(grid.numpy(), (1,2,0)), cmap='gray')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# B. Model (Conv Autoencoder)\n",
        "# - U-Net–like AE with light skip connections\n",
        "# - SiLU activations + GroupNorm (stable)\n",
        "# - Output uses `tanh` to match inputs normalized to [-1, 1]\n",
        "\n",
        "# B.1 Define model\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, cin, cout):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(cin, cout, 3, padding=1),\n",
        "            nn.GroupNorm(8, cout),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(cout, cout, 3, padding=1),\n",
        "            nn.GroupNorm(8, cout),\n",
        "            nn.SiLU(),\n",
        "        )\n",
        "        self.pool = nn.Conv2d(cout, cout, 2, stride=2)  # downsample by 2\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        y = self.pool(x)\n",
        "        return x, y   # return pre-pooled for skip\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_c, skip_c, out_c):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_c, out_c, 2, stride=2)  # upsample by 2\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(out_c + skip_c, out_c, 3, padding=1),\n",
        "            nn.GroupNorm(8, out_c),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(out_c, out_c, 3, padding=1),\n",
        "            nn.GroupNorm(8, out_c),\n",
        "            nn.SiLU(),\n",
        "        )\n",
        "    def forward(self, x, skip):\n",
        "        x = self.up(x)\n",
        "        # Defensive: if shapes differ by 1, center-crop the skip\n",
        "        if x.shape[-2:] != skip.shape[-2:]:\n",
        "            dh = skip.shape[-2] - x.shape[-2]\n",
        "            dw = skip.shape[-1] - x.shape[-1]\n",
        "            skip = skip[:, :, dh//2:skip.shape[-2]-(dh - dh//2),\n",
        "                             dw//2:skip.shape[-1]-(dw - dw//2)]\n",
        "        x = torch.cat([x, skip], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "class ConvAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.inp = nn.Conv2d(1, 32, 3, padding=1)\n",
        "\n",
        "        # Encoder (2 downs: 28→14→7)\n",
        "        self.d1 = Down(32, 64)     # s1: 64 ch @ 28×28\n",
        "        self.d2 = Down(64, 128)    # s2: 128 ch @ 14×14\n",
        "\n",
        "        # Bottleneck on 7×7, expand channels for capacity\n",
        "        self.to_bot = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.mid = nn.Sequential(\n",
        "            nn.GroupNorm(16, 256),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),\n",
        "            nn.GroupNorm(16, 256),\n",
        "            nn.SiLU(),\n",
        "        )\n",
        "\n",
        "        # Decoder (2 ups: 7→14→28)\n",
        "        self.u2 = Up(in_c=256, skip_c=128, out_c=128)  # 7→14, merge with s2\n",
        "        self.u1 = Up(in_c=128, skip_c=64,  out_c=64)   # 14→28, merge with s1\n",
        "\n",
        "        # Output head\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Conv2d(64, 32, 3, padding=1),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(32, 1, 1),\n",
        "            nn.Tanh(),  # match inputs normalized to [-1,1]\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.inp(x)            # 32 @ 28×28\n",
        "        s1, x = self.d1(x)         # s1: 64 @ 28×28, x: 64 @ 14×14\n",
        "        s2, x = self.d2(x)         # s2: 128 @ 14×14, x: 128 @ 7×7\n",
        "        z = self.to_bot(x)         # 256 @ 7×7\n",
        "        z = self.mid(z)            # 256 @ 7×7\n",
        "        return z, (s1, s2)\n",
        "\n",
        "    def decode(self, z, skips):\n",
        "        s1, s2 = skips\n",
        "        x = self.u2(z, s2)         # 128 @ 14×14\n",
        "        x = self.u1(x, s1)         # 64  @ 28×28\n",
        "        return self.out(x)         # 1   @ 28×28\n",
        "\n",
        "    def forward(self, x):\n",
        "        z, skips = self.encode(x)\n",
        "        return self.decode(z, skips)\n",
        "\n",
        "# Recreate model and sanity check shapes\n",
        "model = ConvAE().to(device)\n",
        "print(\"Model params (M):\", sum(p.numel() for p in model.parameters())/1e6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# B.2 quick shape test\n",
        "model.eval()\n",
        "xb, _ = next(iter(train_loader))\n",
        "xb = xb.to(device)[:8]\n",
        "with torch.no_grad():\n",
        "    xhat = model(xb)\n",
        "print(\"Input shape :\", tuple(xb.shape))\n",
        "print(\"Output shape:\", tuple(xhat.shape))\n",
        "assert xhat.shape == xb.shape, \"AE output must match input shape (N,1,28,28)\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# C. Training\n",
        "# - Optimizer: AdamW\n",
        "# - Scheduler: CosineAnnealingWarmRestarts\n",
        "# - Mixed precision (AMP)\n",
        "# - Early stopping on **validation Relative L2**\n",
        "# - Saves:\n",
        "#     - `best_model.pt`  (weights only)\n",
        "#     - `best_val.ckpt`  (full checkpoint incl. optimizer)\n",
        "#     - `last.ckpt`      (final state)\n",
        "#     - `history.json`   (training curves)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#C.1 Setup optimizer / losses / scheduler / scaler\n",
        "import numpy as np\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.LR, weight_decay=cfg.WEIGHT_DECAY)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1)\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=cfg.AMP)\n",
        "\n",
        "def recon_loss(xhat, x):\n",
        "    \"\"\"Hybrid reconstruction loss: 0.5*L1 + 0.5*L2 (good balance for crispness).\"\"\"\n",
        "    l1 = F.l1_loss(xhat, x)\n",
        "    l2 = F.mse_loss(xhat, x)\n",
        "    return 0.5*l1 + 0.5*l2, l1.item(), l2.item()\n",
        "\n",
        "def batch_relative_l2(xhat, x, eps=1e-12):\n",
        "    \"\"\"Mean over batch of ||x - x̂||2 / ||x||2.\"\"\"\n",
        "    diff = (xhat - x)\n",
        "    num = torch.sqrt((diff**2).flatten(1).sum(1))\n",
        "    den = torch.sqrt((x**2).flatten(1).sum(1) + eps)\n",
        "    return (num / den).mean()\n",
        "\n",
        "# paths to save\n",
        "best_path_weights = exp_dir / \"best_model.pt\"\n",
        "best_path_ckpt    = exp_dir / \"best_val.ckpt\"\n",
        "last_path_ckpt    = exp_dir / \"last.ckpt\"\n",
        "\n",
        "def save_ckpt(path, model, optimizer, epoch, metrics: dict):\n",
        "    torch.save({\n",
        "        \"epoch\": epoch,\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "        \"metrics\": metrics,\n",
        "        \"cfg\": vars(cfg),\n",
        "    }, path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# C.2 Train with early stopping on val Relative L2\n",
        "best_val = float(\"inf\")\n",
        "no_improve = 0\n",
        "history = {\"train_loss\": [], \"val_relL2\": []}\n",
        "\n",
        "for epoch in range(1, cfg.EPOCHS + 1):\n",
        "    # ---- Train ----\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=cfg.AMP):\n",
        "            xhat = model(xb)\n",
        "            loss, _, _ = recon_loss(xhat, yb)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        train_losses.append(loss.item())\n",
        "    scheduler.step(epoch)\n",
        "\n",
        "    # ---- Validate (Relative L2) ----\n",
        "    model.eval()\n",
        "    val_metrics = []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            xhat = model(xb)\n",
        "            val_metrics.append(batch_relative_l2(xhat, yb).item())\n",
        "    val_relL2 = float(np.mean(val_metrics)) if val_metrics else float(\"inf\")\n",
        "    train_loss = float(np.mean(train_losses)) if train_losses else float(\"inf\")\n",
        "\n",
        "    history[\"train_loss\"].append(train_loss)\n",
        "    history[\"val_relL2\"].append(val_relL2)\n",
        "    print(f\"Epoch {epoch:03d} | train {train_loss:.5f} | val RelL2 {val_relL2:.5f}\")\n",
        "\n",
        "    # ---- Early stopping & best checkpoint ----\n",
        "    if best_val - val_relL2 > cfg.MIN_DELTA:\n",
        "        best_val = val_relL2\n",
        "        no_improve = 0\n",
        "        # save both full ckpt and weights-only\n",
        "        save_ckpt(best_path_ckpt, model, optimizer, epoch, {\"val_relL2\": val_relL2})\n",
        "        torch.save(model.state_dict(), best_path_weights)\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= cfg.PATIENCE:\n",
        "            print(\"Early stopping.\")\n",
        "            break\n",
        "\n",
        "# Save last & history\n",
        "save_ckpt(last_path_ckpt, model, optimizer, epoch, {\"val_relL2\": val_relL2})\n",
        "with open(exp_dir / \"history.json\", \"w\") as f:\n",
        "    json.dump(history, f, indent=2)\n",
        "\n",
        "# Load best weights for downstream evaluation\n",
        "model.load_state_dict(torch.load(best_path_weights, map_location=device))\n",
        "model.eval()\n",
        "print(\"Loaded best weights from:\", best_path_weights.resolve())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#D. Test metrics\n",
        "# Computes Relative L1 and Relative L2 on the **test** set using the *best* checkpoint.\n",
        "\n",
        "# D.1 Ensure best weights are loaded (defensive)\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "try:\n",
        "    _ = best_path_weights\n",
        "except NameError:\n",
        "    best_path_weights = exp_dir / \"best_model.pt\"\n",
        "\n",
        "model.load_state_dict(torch.load(best_path_weights, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "def per_sample_rel_errors(xhat, x, eps=1e-12):\n",
        "    diff = (xhat - x)\n",
        "    relL1 = diff.abs().flatten(1).sum(1) / (x.abs().flatten(1).sum(1) + eps)\n",
        "    relL2 = torch.sqrt((diff**2).flatten(1).sum(1)) / (torch.sqrt((x**2).flatten(1).sum(1) + eps))\n",
        "    return relL1, relL2\n",
        "\n",
        "all_relL1, all_relL2 = [], []\n",
        "with torch.no_grad():\n",
        "    for xb, _ in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        xhat = model(xb)\n",
        "        r1, r2 = per_sample_rel_errors(xhat, xb)\n",
        "        all_relL1.append(r1.cpu()); all_relL2.append(r2.cpu())\n",
        "\n",
        "all_relL1 = torch.cat(all_relL1)\n",
        "all_relL2 = torch.cat(all_relL2)\n",
        "\n",
        "test_relL1 = all_relL1.mean().item()\n",
        "test_relL2 = all_relL2.mean().item()\n",
        "\n",
        "print(f\"Test Relative L1: {test_relL1:.6f}\")\n",
        "print(f\"Test Relative L2: {test_relL2:.6f}\")\n",
        "\n",
        "# Save metrics\n",
        "with open(exp_dir / \"test_metrics.json\", \"w\") as f:\n",
        "    json.dump({\"relL1\": test_relL1, \"relL2\": test_relL2}, f, indent=2)\n",
        "print(\"Saved metrics to:\", (exp_dir / \"test_metrics.json\").resolve())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# E. Visualize worst / median / best\n",
        "# Ranks test samples by Relative L2 and shows side-by-side (original | recon).\n",
        "\n",
        "#E.1 Collect reconstructions and errors over the test set\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "images, recons, errs = [], [], []\n",
        "with torch.no_grad():\n",
        "    for xb, _ in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        xhat = model(xb)\n",
        "        _, r2 = per_sample_rel_errors(xhat, xb)\n",
        "        images.append(xb.cpu()); recons.append(xhat.cpu()); errs.append(r2.cpu())\n",
        "\n",
        "images = torch.cat(images)   # (N,1,28,28)\n",
        "recons = torch.cat(recons)\n",
        "errs   = torch.cat(errs)\n",
        "\n",
        "worst_idx  = torch.argmax(errs).item()\n",
        "best_idx   = torch.argmin(errs).item()\n",
        "median_val = torch.median(errs).item()\n",
        "median_idx = (errs - median_val).abs().argmin().item()\n",
        "\n",
        "print(\"Indices → Best:\", best_idx, \"| Median:\", median_idx, \"| Worst:\", worst_idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# E.2 Plot helper and save images\n",
        "def show_pair(orig, recon, title, save_path=None):\n",
        "    o = inv_normalize(orig).clamp(0,1).squeeze(0)   # (H,W)\n",
        "    r = inv_normalize(recon).clamp(0,1).squeeze(0)  # (H,W)\n",
        "    both = torch.cat([o, r], dim=-1)                # side-by-side\n",
        "\n",
        "    plt.figure(figsize=(3,3))\n",
        "    plt.axis('off')\n",
        "    plt.title(title)\n",
        "    plt.imshow(both, cmap='gray')\n",
        "    if save_path is not None:\n",
        "        plt.savefig(save_path, bbox_inches='tight', dpi=160)\n",
        "    plt.show()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for label, idx in [(\"BEST\", best_idx), (\"MEDIAN\", median_idx), (\"WORST\", worst_idx)]:\n",
        "        x  = images[idx:idx+1]\n",
        "        xr = recons[idx:idx+1]\n",
        "        r1, r2 = per_sample_rel_errors(xr, x)\n",
        "        fp = exp_dir / f\"test_{label.lower()}_pair.png\"\n",
        "        show_pair(x[0], xr[0], f\"{label} — RelL1={r1.item():.4f} | RelL2={r2.item():.4f}\", fp)\n",
        "        print(f\"Saved {label.lower()} pair to:\", fp.resolve())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ae-vae-vqvae",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
